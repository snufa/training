{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, NamedTuple\n",
    "from utils import make_spike_raster_dataset\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import pylab as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nc = 2 # Number of Classes\n",
    "N = [16, 32, Nc] # List of number of neurons per layer\n",
    "Nepochs = 10\n",
    "T = 100\n",
    "NUM_SAMPLES_PER_CLASS = 1000\n",
    "TRAIN_TEST_SPILT = 0.8\n",
    "NUM_SAMPLES_TRAIN = int(Nc*NUM_SAMPLES_PER_CLASS*TRAIN_TEST_SPILT)\n",
    "BATCHSIZE = 48\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TODO not necessary for keras, but is more general, easier to extend...\n",
    "def create_dataloader(data, labels, batchsize, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "    num_samples = labels.shape[0]\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(num_samples, reshuffle_each_iteration=False)\n",
    "    # dataset = dataset.repeat()\n",
    "    # dataset = dataset.interleave(num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batchsize, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "data, labels = make_spike_raster_dataset(rng, nb_classes=Nc, nb_units=N[0], nb_steps=T, step_frac=1.0, dim_manifold=2, nb_spikes=1, nb_samples=NUM_SAMPLES_PER_CLASS, alpha=2.0, shuffle=True)\n",
    "data_train, labels_train = data[:NUM_SAMPLES_TRAIN], labels[:NUM_SAMPLES_TRAIN]\n",
    "data_test,  labels_test  = data[NUM_SAMPLES_TRAIN:], labels[NUM_SAMPLES_TRAIN:]\n",
    "dataloader_train = create_dataloader(data_train, labels_train, BATCHSIZE, shuffle=True)\n",
    "dataloader_test  = create_dataloader(data_test,  labels_test,  BATCHSIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def smooth_step(x):\n",
    "  spikes = tf.experimental.numpy.heaviside(x, 1)\n",
    "  beta = 10.0\n",
    "  \n",
    "  def grad(upstream):\n",
    "    return upstream * 1/(beta*tf.math.abs(x)+1)**2\n",
    "  return spikes, grad    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFDenseNeuronState(NamedTuple):\n",
    "    '''\n",
    "    Generic Module for storing the state of an RNN/SNN.\n",
    "    '''\n",
    "    U: Union[tf.Tensor, tf.TensorShape]\n",
    "    I: Union[tf.Tensor, tf.TensorShape]\n",
    "    Ir: Union[tf.Tensor, tf.TensorShape]\n",
    "    S: Union[tf.Tensor, tf.TensorShape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_init(shape, dtype=None):\n",
    "    limit = (6/(shape[0]+shape[1]))**0.5\n",
    "    return tf.random.uniform(shape, minval=-limit, maxval=limit, dtype=dtype)\n",
    "\n",
    "class LIFDensePopulation(keras.layers.Layer):\n",
    "    def __init__(self, out_channels, alpha, beta, betar):\n",
    "        super().__init__()\n",
    "        # choose your initialization method...\n",
    "        self.fc_layer  = keras.layers.Dense(out_channels, use_bias=False, kernel_initializer=keras.initializers.RandomUniform(-0.5, 0.5))\n",
    "        # self.fc_layer  = keras.layers.Dense(out_channels, use_bias=False, kernel_initializer=keras.initializers.GlorotUniform())\n",
    "        # self.fc_layer  = keras.layers.Dense(out_channels, use_bias=False, kernel_initializer=custom_init)\n",
    "        self.rec_layer = keras.layers.Dense(out_channels, use_bias=False, kernel_initializer=keras.initializers.Constant(0.0))\n",
    "        self.out_channels = out_channels\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.betar = betar\n",
    "   \n",
    "    def call(self, Sin_t, state):\n",
    "        U = self.alpha*(1-tf.stop_gradient(state.S))*state.U + (1-self.alpha)*(20*state.I+state.Ir)\n",
    "        I = self.beta*state.I + (1-self.beta)*self.fc_layer(Sin_t)\n",
    "        Ir = self.betar*state.Ir + (1-self.betar)*self.rec_layer(state.S)\n",
    "        S = smooth_step(U-1)\n",
    "        new_state = LIFDenseNeuronState(U, I, Ir, S)\n",
    "        return S, new_state\n",
    "    \n",
    "    def get_initial_state(self, inputs, batch_size, dtype):\n",
    "        return LIFDenseNeuronState(*[tf.zeros((batch_size, self.out_channels)) for _ in range(4)])\n",
    "\n",
    "    def get_state_size(self):\n",
    "        return LIFDenseNeuronState(*[tf.TensorShape((self.out_channels,)) for _ in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFNetworkCell(keras.layers.Layer):\n",
    "    def __init__(self, N, alpha = .95, beta = .9, betar = .85):\n",
    "        super().__init__()        \n",
    "        layers = []\n",
    "        for units in N[1:]:\n",
    "            layers.append(LIFDensePopulation(out_channels = units,\n",
    "                                             alpha = alpha,\n",
    "                                             beta = beta,\n",
    "                                             betar = betar))\n",
    "        self.layers = layers\n",
    "        self.state_size = [layer.get_state_size() for layer in self.layers]\n",
    "        # self.output_size = [layer.get_state_size() for layer in self.layers]\n",
    "        self.output_size = [layer.get_state_size().S for layer in self.layers]\n",
    "    \n",
    "    def call(self, Sin_t, state):\n",
    "        new_state = []\n",
    "        for layer,state_ilay in zip(self.layers, state):\n",
    "            Sin_t, new_state_ilay = layer(Sin_t, state_ilay)\n",
    "            new_state.append(new_state_ilay)\n",
    "        return [stat.S for stat in new_state], new_state # Returns final state of last layer\n",
    "\n",
    "def model_fn(seq_len, batchsize, dims, alpha = .95, beta = .9, betar = .85, return_sequences=True):\n",
    "    inp_spikes = keras.Input(shape=(seq_len, dims[0]), batch_size=batchsize, dtype=tf.float32)\n",
    "    out = keras.layers.RNN(LIFNetworkCell(dims, alpha, beta, betar), return_sequences=return_sequences, time_major=False)(inp_spikes)\n",
    "    return inp_spikes, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = keras.Model(*model_fn(T, BATCHSIZE, N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_and_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    sum_spikes = tf.reduce_sum(y_pred, axis=1) # (batch, seq_len, neurons)\n",
    "    softmax_pred = tf.nn.softmax(sum_spikes, axis=1)\n",
    "    one_hot_target = tf.one_hot(y_true, softmax_pred.shape[-1], axis=-1, dtype=softmax_pred.dtype)\n",
    "    return tf.math.reduce_sum((softmax_pred-one_hot_target)**2)/y_true.shape[-1]\n",
    "    # return tf.keras.metrics.sparse_categorical_crossentropy(y_true, sum_spikes, from_logits=True)\n",
    "\n",
    "def calc_activity(y_true, y_pred):\n",
    "    sum_spikes = tf.reduce_sum(y_pred) /(y_pred.shape[0]*y_pred.shape[2])\n",
    "    return sum_spikes\n",
    "\n",
    "def calc_accuracy(y_true, y_pred):\n",
    "    sum_spikes = tf.reduce_sum(y_pred, axis=1)\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, sum_spikes)\n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(learning_rate=1e-1, momentum=0.9, name=\"SGD\")\n",
    "\n",
    "num_layers = len(N)-1\n",
    "out_name = net.layers[-1].name\n",
    "# only calculate loss for last layer\n",
    "loss_funcs = {f\"{out_name}_{i}\" if i>0 else out_name : lambda x, y: 0.0 for i in range(num_layers-1)}\n",
    "loss_funcs[f\"{out_name}_{num_layers-1}\"] = sum_and_sparse_categorical_crossentropy\n",
    "\n",
    "# calculate activity of all layers and additionally accuracy of last layer\n",
    "metrics = {f\"{out_name}_{i}\" if i>0 else out_name: calc_activity for i in range(num_layers)}\n",
    "metrics[f\"{out_name}_{num_layers-1}\"] = [calc_activity, calc_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.compile(opt, loss_funcs,\n",
    "            metrics=metrics,\n",
    "            steps_per_execution=24, # Execute multiple batches with a single call\n",
    "            # jit_compile=True # jit compile the model for faster execution\n",
    ")\n",
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fit(dataloader_train, epochs=Nepochs, workers=BATCHSIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "09badfc81b13487db2d931fd2e7fad7412098b1da4ee53fdee2eaf19dc2937cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
