{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Implementation of a Spiking Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to provide a simple tutorial on how to implement spiking neural networks in the deep learning framework PyTorch. For more information on PyTorch, see https://pytorch.org/.\n",
    "The spiking neural network will be implemented from scratch using only features available in basic PyTorch (apart from some simple utils and a package to generate data). The spiking neural network will be trained using gradient descent together with the surrogate gradient descent method to leverage the spiking discontinuities that inevitably arise. You can find more on spiking neural networks under https://neuronaldynamics.epfl.ch/online/. The tutorial requires some prior experience with training artificial neural networks as well as a basic understanding of spiking neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "First, we import some packages that are helpful in implementing spiking neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import tqdm\n",
    "from utils import make_spike_raster_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # change to 'cpu' if no NVIDIA GPU is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "We start by defining the shape of our neural network. For simplicity, we start only with a couple of fully connected neural networks. Will work with a popular toy dataset where points are sampled from random manifolds, called the \"Randman\". We embed these d-dimensional manifolds in an n-dimensional space and then sample points from them which serve as\n",
    "To learn more about random manifolds, look at https://direct.mit.edu/neco/article/33/4/899/97482/The-Remarkable-Robustness-of-Surrogate-Gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 2 # Number of classes\n",
    "N = [16, 32, N_CLASSES] # List of number of neurons per layer\n",
    "N_EPOCHS = 25 # Number of training epochs\n",
    "T = 100 # Number of timesteps per epoch\n",
    "NUM_SAMPLES_PER_CLASS = 1000 \n",
    "TRAIN_TEST_SPILT = 0.8\n",
    "NUM_SAMPLES_TRAIN = int(N_CLASSES*NUM_SAMPLES_PER_CLASS*TRAIN_TEST_SPILT)\n",
    "BATCHSIZE = 48\n",
    "\n",
    "SEED = 42 \n",
    "rng = np.random.default_rng(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Creation\n",
    "\n",
    "We start by defining a dataloader for our experiment, load the data and split it into a train and a test dataset. The dataset we analyze is called the Random Manifolds or Randman dataset, which consists of d-dimensional manifolds embedded into a n-dimensional space. You can find more about this dataset under https://github.com/fzenke/randman ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandmanDataset(Dataset):\n",
    "    \"\"\"Characterizes a PyTorch dataset for use with the PyTorch dataloader.\"\"\"\n",
    "    def __init__(self, data, labels):\n",
    "        \"\"\"Simple initialization of the given dataset.\"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Retrives a single sample from the given dataset.\"\"\"\n",
    "        # Load data and get label\n",
    "        X = self.data[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "data, labels = make_spike_raster_dataset(rng, nb_classes=N_CLASSES, nb_units=N[0], \n",
    "                    nb_steps=T, step_frac=1.0, dim_manifold=2, nb_spikes=1, \n",
    "                    nb_samples=NUM_SAMPLES_PER_CLASS, alpha=2.0, shuffle=True)\n",
    "\n",
    "data_train, labels_train = data[:NUM_SAMPLES_TRAIN], labels[:NUM_SAMPLES_TRAIN]\n",
    "data_test,  labels_test  = data[NUM_SAMPLES_TRAIN:], labels[NUM_SAMPLES_TRAIN:]\n",
    "\n",
    "dataset_train = RandmanDataset(data_train, labels_train)\n",
    "dataset_test = RandmanDataset(data_test, labels_test)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCHSIZE, shuffle=True, drop_last=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCHSIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "In this section, we define the various components that are needed to create a differential layer of stateful neurons, such that we are able to construct an arbitrary neural network consisting of fully connected spiking neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Function and Surrogate Gradient Methods\n",
    "In this section, we create a smooth version of the Heaviside step function $\\Theta(x)$ such that the gradient is defined at every point. This surrogate function for the backward pass looks like:\n",
    "$\n",
    "\\Theta'(x) \\equiv \\dfrac{1}{(\\beta \\cdot |x| + 1)^2}\n",
    "$<br>\n",
    "This choice of a surrogate gradient is called \"superspike\" and is quite popular in the neuromorphic computing community. However, the model performance is robust against the actual functional form of the surrogate, see https://direct.mit.edu/neco/article/33/4/899/97482/The-Remarkable-Robustness-of-Surrogate-Gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothStep(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here, we define a surrogate gradient for the Heaviside step function.\n",
    "    https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(aux, x):\n",
    "        aux.save_for_backward(x)\n",
    "        return (x >= 0).float() # Behavior similar to Heaviside step function\n",
    "\n",
    "    def backward(aux, grad_output): # Define the behavior for the backward pass \n",
    "        beta = 10.0\n",
    "        input, = aux.saved_tensors\n",
    "        surrogate = 1.0/(beta*torch.abs(input) + 1.0)**2\n",
    "        grad_input = grad_output.clone() * surrogate\n",
    "        \n",
    "        return grad_input\n",
    "    \n",
    "smooth_step = SmoothStep().apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the Stateful Layer of Spiking Neurons within the Pytorch Framework\n",
    "Here, we define a new type of layer that inherits from the abstract class nn.Module, such that it can be used like other, ordinary NN layers withing the PyTorch framework. In particular, we have to create buffers for the different state variables of the neurons, i.e. the membrane potential $U$, the current $I$, the recurrent current $I_r$ and the spikes $S$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFDenseNeuronState(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic module for storing the state of an RNN/SNN.\n",
    "    We use the buffer function of torch nn.Module to register our\n",
    "    different states such that PyTorch can manage them.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"Simple initialization of the internal states of a LIF population.\"\"\"\n",
    "        super(LIFDenseNeuronState, self).__init__()\n",
    "        self.state_names = ['U', 'I', 'Ir', 'S']\n",
    "        self.register_buffer('U',torch.zeros(1, out_channels), persistent=True)\n",
    "        self.register_buffer('I',torch.zeros(1, out_channels), persistent=True)\n",
    "        self.register_buffer('Ir',torch.zeros(1, out_channels), persistent=True)\n",
    "        self.register_buffer('S',torch.zeros(1, out_channels), persistent=True)\n",
    "                                                    \n",
    "    def update(self, **values):\n",
    "        \"\"\"Function to update the internal states.\"\"\"\n",
    "        for k, v in values.items():\n",
    "            setattr(self, k, v) \n",
    "    \n",
    "    def init(self, v=0): \n",
    "        \"\"\"Function that detaches the state/graph across trials.\"\"\"\n",
    "        for k in self.state_names:\n",
    "            state = getattr(self, k)\n",
    "            setattr(self, k, torch.zeros_like(state)+v)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the Neuronal Dynamics\n",
    "The following class defines the dynamics of a layer of spiking neurons according to the leaky integrate-and-fire (LIF) formalism. The differential equations read:<br>\n",
    "$\n",
    "\\frac{\\mathrm{d}U^{(l)}_i}{\\mathrm{d}t} = -\\frac{1}{\\tau_\\mathrm{mem}}((U_i^{(l)} - U_\\mathrm{rest}) + R(I_i^{(l)} + I_{i,r}^{(l)})) + (U_i^{(l)} - \\vartheta)S_i^{(l)}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\mathrm{d}I_i^{(l)}}{\\mathrm{d}t} = -\\frac{I_i^{(l)}}{\\tau_\\mathrm{syn}} + \\sum_j W^{(l)}_{ij} S_j^{(l-1)}\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\mathrm{d}I_{i, \\mathrm{r}}^{(l)}}{\\mathrm{d}t} = -\\frac{I_{i, \\mathrm{r}}^{(l)}}{\\tau_\\mathrm{syn,r}} + \\sum_j V_{ij}^{(l)}S_j^{(l)}\n",
    "$\n",
    "\n",
    "Here, $U_i^{(l)}$ is the membrane potential of the $i$th neuron in the layer $l$, $I_i^{(l)}$ is its individual feed-forward input current and $S_i^{(l-1)}$, $S_i^{(l)}$ contain the incoming spikes from the prior layer and emitted spikes form the current layer. \n",
    "Thus, the variable $I_{i, \\mathrm{r}}^{(l)}$ contains the current coming from recurrent connections within the layer and the weight matrices $W_{ij}^{(l)}$ and $V_{ij}^{(l)}$ contain the weights of the feed-forward and recurrent connections respectively.\n",
    "The parameters $\\tau_\\mathrm{mem}$, $\\tau_{syn}$ and $\\tau_{syn,r}$ are essentially decay constants that control the \"leak\" of the neurons, while $\\vartheta$ and $U_{rest}$ are the spiking threshold and resting potential of the neurons in this layer respectively. \n",
    "For more on this topic, see https://arxiv.org/pdf/1901.09948.pdf. <br>\n",
    "Pytorch by itself is not able to work with differential equations, so we need to discretize them.\n",
    "They can be discretized using the forward Euler scheme such that we arrive at equations that we can implement into our pytorch layer: <br>\n",
    "$\n",
    "U_i^{(l)}[n+1] = \\alpha U_i^{(l)}[n] (1 - S_j^{(l)}) + (1 - \\alpha) (I_i^{(l)}[n] + I_{i, \\mathrm{r}}^{(l)}[n])\n",
    "$\n",
    "\n",
    "$\n",
    "I_i^{(l)}[n+1] = \\beta I_i^{(l)}[n] + (1 - \\beta) \\sum_j W^{(l)}_{ij} S_j^{(l-1)}[n]\n",
    "$\n",
    "\n",
    "$\n",
    "I_{i, \\mathrm{r}}^{(l)}[n+1] = \\beta_\\mathrm{r} I_{i, \\mathrm{r}}^{(l)}[n] + (1 - \\beta_\\mathrm{r}) \\sum_j V_{ij}^{(l)}S_j^{(l)}[n]\n",
    "$\n",
    "\n",
    "To implement this formula, we make use of the already present nn.Linear layer in pytorch to store the feed-forward and recurrent connection weights. Also, we set the parameters $\\vartheta = 1$, $R = 1$ and $U_\\mathrm{rest} = 0$.\n",
    "Note that this definition of the neural dynamics implies that we interpret our spiking neural network as a recurrent neural network, such that when we train it, we will have to use backpropagation through time (BPTT). This makes training spiking neural networks much more resource-demanding than training a compatible artificial neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFDensePopulation(nn.Module):    \n",
    "    def __init__(self, in_channels, out_channels, alpha = .95, beta=.9, betar=.85, theta=1.0):\n",
    "        super(LIFDensePopulation, self).__init__()       \n",
    "        \"\"\"\n",
    "        Function to initialize a layer of leaky integrate-and-fire neurons.\n",
    "        \"\"\"\n",
    "        self.fwd_layer = nn.Linear(in_channels, out_channels) # Used to store feed-forward weights\n",
    "        self.rec_layer = nn.Linear(out_channels, out_channels, bias=False) # Used to store recurrent weights\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.alpha = alpha # Controls decay of membrane potential\n",
    "        self.beta = beta # Controls decay of feed-forward input current\n",
    "        self.betar = betar # Controls decay of recurrent input current\n",
    "        self.theta = theta\n",
    "        self.state = LIFDenseNeuronState(self.in_channels, self.out_channels)\n",
    "        \n",
    "        # lim = (6/(in_channels+out_channels))**0.5\n",
    "        self.fwd_layer.weight.data.uniform_(-.3, .3) # Initialization of feed-forward layer\n",
    "        self.rec_layer.weight.data.uniform_(-.0, .0) # Recurrent layer is initialized with zero weights, i.e. ignored\n",
    "        self.fwd_layer.bias.data.uniform_(-.01, .01) # Initialization of a slight bias current for the fc layer\n",
    "        \n",
    "    def forward(self, Sin_t):\n",
    "        \"\"\"Forward pass of a batch through the data.\"\"\"\n",
    "        state = self.state\n",
    "        U = self.alpha*(1-state.S.detach())*state.U + (1-self.alpha)*(state.I+state.Ir) # I is weighted by a factor of 20\n",
    "        I = self.beta*state.I + (1-self.beta)*self.fwd_layer(Sin_t)\n",
    "        Ir = self.betar*state.Ir + (1-self.betar)*self.rec_layer(state.S)\n",
    "        S = smooth_step(U-self.theta)\n",
    "        # Update the neuronal state\n",
    "        self.state.update(U=U, I=I, Ir=Ir, S=S)\n",
    "        return self.state\n",
    "    \n",
    "    def init_state(self, value=0):\n",
    "        \"Initialize the state variables of this layer.\"\n",
    "        self.state.init(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the Network\n",
    "This class uses a loop to concstruct multiple layers of fully connected layers of spiking neurons according to the parameters given in the array $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFNetwork(nn.Module):\n",
    "    def __init__(self, N, alpha=.95, beta=.9, betar=.85, theta=1.0):\n",
    "        \"\"\"\n",
    "        Function to initialize a spiking neural network consisting of multiple \n",
    "        layers of spiking neural networks.\n",
    "        \"\"\"\n",
    "        super(LIFNetwork, self).__init__()        \n",
    "        layers = []\n",
    "        for i in range(len(N)-1):\n",
    "            layers.append(LIFDensePopulation(in_channels=N[i],\n",
    "                                             out_channels=N[i+1],\n",
    "                                             alpha=alpha,\n",
    "                                             beta=beta,\n",
    "                                             betar=betar,\n",
    "                                             theta=theta))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def step(self, Sin_t):    \n",
    "        \"\"\"\n",
    "        Perform a single batched forward timestep through the network.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            Sin_t = layer(Sin_t).S\n",
    "        return layer.state # Returns final state of last layer\n",
    "    \n",
    "    def forward(self, Sin):\n",
    "        \"\"\"Complete batched forward pass through the network.\"\"\"\n",
    "        self.init_state()\n",
    "        for t in range(Sin.shape[0]):\n",
    "            state = net.step(Sin[t].view(-1, N[0]))\n",
    "        return state\n",
    "       \n",
    "    def init_state(self):\n",
    "        \"\"\"Initialize states of the network.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.init_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the dynamics are expressed within the PyTorch framework, we are able to simply move the device to a GPU for training, which is usually much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LIFNetwork(N).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following does optional data-driven initialization to improve learning. This example uses the LSUV (layer-sequential unit variance) initialization from https://arxiv.org/abs/1511.06422. A good initialization is particularly important for spiking neural networks to make sure that we have a balance between the spiking count of all the neurons and no quiescent neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import forward_layer, torch_init_LSUV\n",
    "data, target = next(iter(dataloader_train))\n",
    "data_flat =  data.transpose(0,1).to(device).view(T,BATCHSIZE,-1)\n",
    "torch_init_LSUV(net, data_flat, tgt_mu=-0.75) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working on a classification task, it makes sense to use a cross-entropy loss to train the model. The output of the model is typically a sequence of frames containing zeros and ones to indicate whether a neuron has spiked at a certain timestep. These spikes are then summed up and we apply a softmax for them to get a probability distribution for the different classes. Thus output neuron that spiked the most gives the highest probability for the corresponding class. Encoding information in this way is called rate-coding/spike count. Furthermore we will use an ADAM optimizer to adjust the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(net.parameters(), lr=2e-2) #, momentum=0.9) # , betas=[0., .95]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this programming block, we define the training loop for our spiking neural network. As we have provided a proper gradient for the spiking discontinuities, we can train our model using gradient descent and all the other features that are available in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm.trange(N_EPOCHS)\n",
    "for epoch in pbar: \n",
    "    loss = 0\n",
    "    acc = []\n",
    "    # Test loop\n",
    "    for Sin, target in dataloader_test: \n",
    "        Sin = Sin.to(device).transpose(0,1)\n",
    "        target = target.to(device)\n",
    "        with torch.no_grad(): # Do not change weights during testing\n",
    "            state = net(Sin)\n",
    "            tgt = target.cpu().numpy()\n",
    "            pred = state.S.clone().cpu().numpy()\n",
    "            acc.append(np.mean(np.argmax(pred, axis=1)==tgt))\n",
    "    # Training loop\n",
    "    for Sin, target in dataloader_train:\n",
    "        Sin = Sin.to(device).transpose(0,1)\n",
    "        target = target.to(device).long() # PyTorch loss function requires Long-type\n",
    "        state = net(Sin)\n",
    "        loss_seq = loss_fn(state.S, target)\n",
    "        loss_seq.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        loss += loss_seq\n",
    "\n",
    "    pbar.set_description(\"Training Loss {0} | Accuracy {1:2.2%}: | \" .format(loss,np.mean(acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probe and plot final layer states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some intuition about how the different layers behave, we can take a look at the evolution of the membrane potential and the spiking behavior of the output neurons by analyzing a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('snn_utils.py'):\n",
    "    !wget https://raw.githubusercontent.com/surrogate-gradient-learning/pytorch-lif-autograd/master/snn_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snn_utils # very helpful for visualization of SNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uprobe = np.empty([BATCHSIZE,T,N[-1]])\n",
    "Iprobe = np.empty([BATCHSIZE,T,N[-1]])\n",
    "Sprobe = np.empty([BATCHSIZE,T,N[-1]])\n",
    "for t in range(T):\n",
    "    state = net(data_flat[t].cuda())\n",
    "    Uprobe[:,t] = state.U.clone().data.cpu().numpy()\n",
    "    Iprobe[:,t] = state.I.clone().data.cpu().numpy()\n",
    "    Sprobe[:,t] = state.S.clone().data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn_utils.plotLIF(U=Iprobe[0], S=Sprobe[0], staggering = .2);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "09badfc81b13487db2d931fd2e7fad7412098b1da4ee53fdee2eaf19dc2937cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
